# Research Interests

I have the good fortune of being a [research scientist at IBM Research](https://research.ibm.com/people/parikshit-ram) where I am currently considering various algorithms and applications of **(mostly stochastic) bilevel optimization in AI and machine learning** and trying to theoretically understand the various properties of transformer based models such as **in-context learning and compositional generalization**. You can find more about this [here](./research/bilevel.md), [here](./research/compgen.md) and [here](./research/icl.md). I have also studied the bilevel problem(s) appearing in **Automated Machine Learning and AI (AutoML and AutoAI)**, developing various new (continuous and discrete) derivative-free algorithms and also understanding various theoretical aspects of the AutoML/AutoAI problem (more details [here](./research/autoai.md)). I have also been lucky to be involved with the [**Lale**](https://github.com/IBM/lale) project on "Gradual AutoML", which allows one to succinctly craft sophisticated ML pipelines, and automate the search of these pipelines.

I also moonlight as a (ab)user of neuro-inspired algorithmic motifs, especially really high dimensional but really sparse latent representations, for novel "simpler" ways of learning (see [here](./research/neuro.md)). I am also interested in the unsupervised problem of [density estimation](./research/dest.md), both from a perspective of interpretability and computational efficiency. I have spent a good amount of time studying various [all-pairs problems](./research/cgallpairs.md) in computational geometry such as similarity search, range search, and density estimation, which have a naive quadratic computational complexity in the number of points, and we have explored algorithms that have both empirical and theoretical complexity as close as possible to linear. My [thesis research](./research/papers/2013/RAM-DISSERTATION-2013.pdf) was the related topic of [similarity search](./research/simsearch.md), and how we can both (i) use fast similarity search to speed up ML, and (ii) how ML can be used to speed up similarity search.


## Publications

Here is a (loose) categorization of my publications in the different areas:
- [Bilevel Optimization](./research/bilevel.md)
- [Automated Machine Learning / Artificial Intelligence / Data Science](./research/autoai.md)
- [Neuro-Inspired Learning](./research/neuro.md)
- [Compositional Generalization](./research/compgen.md)
- [In-Context Learning](./research/icl.md)
- [Machine Unlearning](./research/unlearn.md)
- [Density Estimation](./research/dest.md)
- [Computation Geometry / All-Pairs Problem](./research/cgallpairs.md)
- [Similarity Search](./research/simsearch.md)

You can find a chronologically sorted list of my publications on my [Google Scholar profile](https://scholar.google.com/citations?hl=en&user=JaXmmnkAAAAJ&sortby=pubdate).


**Experience.**

- Programming languages: Python, C/C++
- Frameworks/libraries: Pytorch, Scikit-Learn, [Lale](https://github.com/IBM/lale), OpenMP, MPI


## News

- (December 2023) I am co-organizing the upcoming NeurIPS 2023 workshop on [Associative Memory & Hopfield Networks](https://amhn.vizhub.ai).
- (August 2023) I recently taught a [session](./research/papers/2023/DR_NSSS2023.slides.pdf) at the [Neuro-Symbolic Summer School 2023](https://neurosymbolic.github.io/nsss2023/).
- (June 2023) I recently taught some sessions at the Polyhedra and Combinatorial Optimization Days 2023 ([JPOC13](https://jpoc13.sciencesconf.org/)) [summer school on Combinatorial Optimization & Machine Learning](https://jpoc13.sciencesconf.org/resource/page/id/4).


## Miscellaneous

- [Biosketch](./biosketch.md)
- Quick access to [tools](./misc/tools.md) I use
- (Current) Reading [list](./misc/rlist.md)
